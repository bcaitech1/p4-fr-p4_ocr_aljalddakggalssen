{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da115edf-38c2-4eee-962e-acbeb212225b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/ml/team/gj/code\n"
     ]
    }
   ],
   "source": [
    "%cd ../code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "959054d7-61e1-4ff2-856f-994c11002f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "import math\n",
    "from attrdict import AttrDict\n",
    "\n",
    "from dataset import (\n",
    "    dataset_loader, SizeBatchSampler, split_gt, load_levels, load_sources, load_vocab, encode_truth\n",
    ")\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2d3ef5c-3cc9-47e4-96ee-cae5924e96aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy\t eval_dataset  rot_model     train_dataset\n",
      "dummy_2  out_stuff     saving_model  val_output.pck\n"
     ]
    }
   ],
   "source": [
    "!ls /opt/ml/input/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09c5bec1-4887-4837-834a-54f2c30bfdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/opt/ml/input/data/train_dataset'\n",
    "image_root = f'{root}/images'\n",
    "source_f = open(f'{root}/source.txt')\n",
    "sources = [line.strip().split('\\t') for line in source_f.readlines()]\n",
    "sources = [(x, int(y)) for x, y in sources]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76f2b86a-f26e-4f59-a1a9-dbbf8a66a401",
   "metadata": {},
   "outputs": [],
   "source": [
    "level_f = open(f'{root}/level.txt')\n",
    "levels = [line.strip().split('\\t') for line in level_f.readlines()]\n",
    "levels = [(x, int(y)) for x, y in levels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bfc9d86-0125-4cce-b272-0d4b77a5b7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_f = open(f'{root}/gt.txt')\n",
    "gts = [line.strip().split('\\t') for line in gt_f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b90dcc00-c05e-48da-8278-10ca823f917e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 100000, 100000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sources_df = pd.DataFrame(sources, columns=['path', 'source'])\n",
    "gts_df = pd.DataFrame(gts, columns=['path', 'gt'])\n",
    "levels_df = pd.DataFrame(levels, columns=['path', 'level'])\n",
    "\n",
    "len(sources_df), len(gts_df), len(levels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5515bf2a-0a82-4ea4-b8ad-1f33732f6165",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = sources_df.merge(levels_df).merge(gts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4ed72e8-28ef-411b-87bb-1bef85a96883",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"Load Dataset\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        groundtruth,\n",
    "        tokens_file,\n",
    "        levels,\n",
    "        sources,\n",
    "        crop=False,\n",
    "        transform=None,\n",
    "        rgb=3,\n",
    "        max_resolution=128*128,\n",
    "        is_flexible=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            groundtruth (string): Path to ground truth TXT/TSV file\n",
    "            tokens_file (string): Path to tokens TXT file\n",
    "            ext (string): Extension of the input files\n",
    "            crop (bool, optional): Crop images to their bounding boxes [Default: False]\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        super(CustomDataset, self).__init__()\n",
    "        self.crop = crop\n",
    "        self.transform = transform\n",
    "        self.rgb = rgb\n",
    "        self.token_to_id, self.id_to_token = load_vocab(tokens_file)\n",
    "        self.data = [\n",
    "            {\n",
    "                \"path\": p,\n",
    "                \"truth\": {\n",
    "                    \"text\": truth,\n",
    "                    \"encoded\": [\n",
    "                        self.token_to_id[START],\n",
    "                        *encode_truth(truth, self.token_to_id),\n",
    "                        self.token_to_id[END],\n",
    "                    ],\n",
    "                    'rotated': np.random.randint(4),\n",
    "                    'flipped': np.random.randint(2),\n",
    "                },\n",
    "            }\n",
    "            for idx, (p, truth) in enumerate(groundtruth)\n",
    "        ]\n",
    "\n",
    "        for datum in self.data:\n",
    "            file_path = datum['path'].split('/')[-1]\n",
    "            source = sources.get(file_path, -100) # -100 crossentory 무시 index\n",
    "            level = levels.get(file_path, -99) - 1 # -100 모름\n",
    "            datum['source'] = source\n",
    "            datum['level'] = level\n",
    "\n",
    "        self.is_flexible = is_flexible\n",
    "        if self.is_flexible:\n",
    "            self.shape_cache = np.zeros((len(self), 2), dtype=int)\n",
    "            self.max_resolution = max_resolution\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        item = self.data[i]\n",
    "        image = Image.open(item[\"path\"])\n",
    "        if self.rgb == 3:\n",
    "            image = image.convert(\"RGB\")\n",
    "        elif self.rgb == 1:\n",
    "            image = image.convert(\"L\")\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        if self.crop:\n",
    "            # Image needs to be inverted because the bounding box cuts off black pixels,\n",
    "            # not white ones.\n",
    "            bounding_box = ImageOps.invert(image).getbbox()\n",
    "            image = image.crop(bounding_box)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.is_flexible:\n",
    "            image = transforms.Resize(self.get_shape(i))(image)\n",
    "            \n",
    "            rot_idx = item['truth']['rotated']\n",
    "        flip_idx = item['truth']['flipped']\n",
    "        \n",
    "        angle = rot_idx * 90\n",
    "        image = transforms.functional.rotate(image, angle)\n",
    "        \n",
    "        if flip_idx == 1:\n",
    "            image = transforms.functional.hflip(image)\n",
    "\n",
    "        return {\n",
    "            \"path\": item[\"path\"],\n",
    "            \"truth\": item[\"truth\"],\n",
    "            \"image\": image,\n",
    "            'source': item['source'],\n",
    "            'level': item['level'],\n",
    "        }\n",
    "\n",
    "    def get_shape(self, i):\n",
    "        h, w = self.shape_cache[i]\n",
    "        if h == 0 and w == 0:\n",
    "            item = self.data[i]\n",
    "            image = Image.open(item[\"path\"])\n",
    "            rw, rh = image.size\n",
    "\n",
    "            T = self.max_resolution\n",
    "            div = rw * rh / T\n",
    "            w = round(rw/math.sqrt(div))\n",
    "            h = round(rh/math.sqrt(div))\n",
    "            w = round(w / 32) * 32\n",
    "            h = T // w\n",
    "            # h = (T // w) // 32 * 32\n",
    "\n",
    "            self.shape_cache[i][0] = h\n",
    "            self.shape_cache[i][1] = w\n",
    "        return h, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f186432-8fb5-4eef-a2f2-bfc306b60dc3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_batch(data):\n",
    "    max_len = max([len(d[\"truth\"][\"encoded\"]) for d in data])\n",
    "    # Padding with -1, will later be replaced with the PAD token\n",
    "    padded_encoded = [\n",
    "        d[\"truth\"][\"encoded\"] + (max_len - len(d[\"truth\"][\"encoded\"])) * [-1]\n",
    "        for d in data\n",
    "    ]\n",
    "    return {\n",
    "        \"path\": [d[\"path\"] for d in data],\n",
    "        \"image\": torch.stack([d[\"image\"] for d in data], dim=0),\n",
    "        \"truth\": {\n",
    "            \"text\": [d[\"truth\"][\"text\"] for d in data],\n",
    "            \"encoded\": torch.tensor(padded_encoded), \n",
    "            'rotated': torch.tensor([d['truth']['rotated'] for d in data]),\n",
    "            'flipped': torch.tensor([d['truth']['flipped'] for d in data]),\n",
    "        },\n",
    "        'level': torch.tensor([d['level'] for d in data], dtype=torch.long),\n",
    "        'source': torch.tensor([d['source'] for d in data], dtype=torch.long),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fc272be-7dd9-4c06-9ec2-3559bb0b7e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = AttrDict(\n",
    "    input_size=AttrDict(\n",
    "        height=128,\n",
    "        width=128\n",
    "    ),\n",
    "    data=AttrDict(\n",
    "        flexible_image_size=True,\n",
    "        random_split=0.2,\n",
    "        train=[\"/opt/ml/input/data/train_dataset/gt.txt\"],\n",
    "        test_proportions=0.2,\n",
    "        dataset_proportions=[1],\n",
    "        use_small_data=False,\n",
    "        token_paths=[\"/opt/ml/input/data/train_dataset/tokens.txt\"],\n",
    "        source_paths=[\"/opt/ml/input/data/train_dataset/source.txt\"],\n",
    "        level_paths=[\"/opt/ml/input/data/train_dataset/level.txt\"],\n",
    "        crop= True,\n",
    "        rgb=1,\n",
    "    ),\n",
    "    batch_size=16,\n",
    "    num_workers=8,\n",
    ")\n",
    "\n",
    "transformed = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# train_data_loader, validation_data_loader, train_dataset, valid_dataset = dataset_loader(options, transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b3dfb10-6812-4921-a718-3eb529f21f6e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Test Data Loading\n",
      "Random Split 0.2\n",
      "From /opt/ml/input/data/train_dataset/gt.txt\n",
      "Prop: 1\tTrain +: 80000\tVal +: 20000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e8e337dd5e5454198478593e14bf275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c119240a45114f9f91ce04b6d57d4125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "START = \"<SOS>\"\n",
    "END = \"<EOS>\"\n",
    "PAD = \"<PAD>\"\n",
    "SPECIAL_TOKENS = [START, END, PAD]\n",
    "\n",
    "\n",
    "train_data, valid_data = [], [] \n",
    "if options.data.random_split:\n",
    "    print('Train-Test Data Loading')\n",
    "    print(f'Random Split {options.data.test_proportions}')\n",
    "    for i, path in enumerate(options.data.train):\n",
    "        prop = 1.0\n",
    "        if len(options.data.dataset_proportions) > i:\n",
    "            prop = options.data.dataset_proportions[i]\n",
    "        train, valid = split_gt(path, prop, options.data.test_proportions)\n",
    "        train_data += train\n",
    "        valid_data += valid\n",
    "        print(f'From {path}')\n",
    "        print(f'Prop: {prop}\\tTrain +: {len(train)}\\tVal +: {len(valid)}')\n",
    "else:\n",
    "    print('Train Data Loading')\n",
    "    for i, path in enumerate(options.data.train):\n",
    "        prop = 1.0\n",
    "        if len(options.data.dataset_proportions) > i:\n",
    "            prop = options.data.dataset_proportions[i]\n",
    "        train = split_gt(path, prop)\n",
    "        train_data += train\n",
    "        print(f'From {path}')\n",
    "        print(f'Prop: {prop}\\tVal +: {len(train)}')\n",
    "\n",
    "    print()\n",
    "    print('Test Data Loading')\n",
    "    for i, path in enumerate(options.data.test):\n",
    "        valid = split_gt(path)\n",
    "        valid_data += valid\n",
    "        print(f'From {path}')\n",
    "        print(f'Val +:\\t{len(valid)}')\n",
    "\n",
    "# Load data\n",
    "if options.data.use_small_data:\n",
    "    old_train_len = len(train_data)\n",
    "    old_valid_len = len(valid_data)\n",
    "    train_data = train_data[:100]\n",
    "    valid_data = valid_data[:10]\n",
    "    print(\"Using Small Data\")\n",
    "    print(f\"Train: {old_train_len} -> {len(train_data)}\")\n",
    "    print(f'Valid: {old_valid_len} -> {len(valid_data)}')\n",
    "\n",
    "levels = load_levels(options.data.level_paths)\n",
    "sources = load_sources(options.data.source_paths)\n",
    "\n",
    "train_dataset = CustomDataset(\n",
    "    train_data, options.data.token_paths, sources=sources,\n",
    "    levels=levels, crop=options.data.crop,\n",
    "    transform=transformed, rgb=options.data.rgb,\n",
    "    max_resolution=options.input_size.height * options.input_size.width,\n",
    "    is_flexible=options.data.flexible_image_size,\n",
    ")\n",
    "\n",
    "valid_dataset = CustomDataset(\n",
    "    valid_data, options.data.token_paths, sources=sources,\n",
    "    levels=levels, crop=options.data.crop,\n",
    "    transform=transformed, rgb=options.data.rgb,\n",
    "    max_resolution=options.input_size.height * options.input_size.width,\n",
    "    is_flexible=options.data.flexible_image_size,\n",
    ")\n",
    "\n",
    "if options.data.flexible_image_size:\n",
    "    train_sampler = SizeBatchSampler(train_dataset, options.batch_size, is_random=True)\n",
    "    train_data_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_sampler=train_sampler,\n",
    "        num_workers=options.num_workers,\n",
    "        collate_fn=collate_batch,\n",
    "    )\n",
    "\n",
    "    valid_sampler = SizeBatchSampler(valid_dataset, options.batch_size, is_random=False)\n",
    "    valid_data_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_sampler=valid_sampler,\n",
    "        num_workers=options.num_workers,\n",
    "        collate_fn=collate_batch,\n",
    "    )\n",
    "else:\n",
    "    train_data_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=options.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=options.num_workers,\n",
    "        collate_fn=collate_batch,\n",
    "    )\n",
    "\n",
    "    valid_data_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=options.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=options.num_workers,\n",
    "        collate_fn=collate_batch,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2295edbb-ab74-43c0-be40-4dd741c544db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks.SATRN import DeepCNN300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28dbaac3-3051-4050-bc17-49786d1f4252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() and use_cuda else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58641dc7-4f15-44a1-9eb1-9d3ad1a13408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc4619d1-6201-4f9d-8203-d9535575f79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotPrediction(nn.Module):\n",
    "    def __init__(self, dropout_rate, rgb=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        output_channel = 300\n",
    "        self.backbone = DeepCNN300(\n",
    "            rgb,\n",
    "            num_in_features=48,\n",
    "            output_channel=output_channel,\n",
    "            dropout_rate=dropout_rate,\n",
    "        )\n",
    "        \n",
    "        self.rotation_head = nn.Linear(output_channel, 4)\n",
    "        self.flip_head = nn.Linear(output_channel, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        \n",
    "        x = torch.sum(x, dim=(-1, -2)) # [B, output_channel]\n",
    "        rot = self.rotation_head(x)\n",
    "        flip = self.flip_head(x)\n",
    "        \n",
    "        return rot, flip\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "265f6657-5325-490f-9111-bfe556054a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rgb = 1\n",
    "\n",
    "model = RotPrediction(\n",
    "    dropout_rate=0.1,\n",
    "    rgb=rgb,\n",
    ") # 출력 대충 [B, output_channel, H//8, W//8]\n",
    "\n",
    "model.to(device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31dcdddf-60c3-4502-81d6-393807c59a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b17f81d-f863-4a2f-80cd-09fb8507ecdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import (\n",
    "    GradScaler, \n",
    "    autocast,\n",
    ")\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9e010e9-fa95-4f96-8229-227b1c0d2eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(\n",
    "    data_loader,\n",
    "    model,\n",
    "    epoch_text,\n",
    "    criterion,\n",
    "    optimizer, \n",
    "    lr_scheduler,\n",
    "    max_grad_norm,\n",
    "    device,\n",
    "#     options,\n",
    "    use_amp=False,\n",
    "    train=True,\n",
    "):\n",
    "    # Disables autograd during validation mode\n",
    "    torch.set_grad_enabled(train)\n",
    "    if train:\n",
    "        model.train()\n",
    "        scaler = GradScaler(enabled=use_amp)\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    losses = []\n",
    "    total_inputs = 0\n",
    "    correct_rotations = 0\n",
    "    correct_flips = 0\n",
    "\n",
    "    with tqdm(\n",
    "        desc=\"{} ({})\".format(epoch_text, \"Train\" if train else \"Validation\"),\n",
    "        total=len(data_loader.dataset),\n",
    "        dynamic_ncols=True,\n",
    "        leave=False,\n",
    "    ) as pbar:\n",
    "        for d in data_loader:\n",
    "            input = d[\"image\"].to(device)\n",
    "\n",
    "            # The last batch may not be a full batch\n",
    "            curr_batch_size = len(input)\n",
    "            exp_rotated = d[\"truth\"][\"rotated\"].to(device)\n",
    "            exp_flipped = d[\"truth\"][\"flipped\"].to(device)\n",
    "            \n",
    "            with autocast(enabled=use_amp):\n",
    "                pred_rot, pred_flip = model(input)\n",
    "\n",
    "                loss_rot = criterion(pred_rot, exp_rotated)\n",
    "                loss_flip = criterion(pred_flip, exp_flipped)\n",
    "                \n",
    "                loss = loss_rot + loss_flip\n",
    "\n",
    "            if train:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                \n",
    "                # Clip gradients, it returns the total norm of all parameters\n",
    "                nn.utils.clip_grad_norm_(\n",
    "                    model.parameters(), max_norm=max_grad_norm\n",
    "                )\n",
    "\n",
    "\n",
    "                # cycle\n",
    "                scaler.step(optimizer)\n",
    "                scale = scaler.get_scale()\n",
    "                scaler.update()\n",
    "                step_scheduler = scaler.get_scale() == scale\n",
    "\n",
    "                if step_scheduler:\n",
    "                    lr_scheduler.step()\n",
    "\n",
    "            losses.append(loss.item() * len(input))\n",
    "            total_inputs += len(input)\n",
    "\n",
    "            prot = torch.argmax(pred_rot, dim=-1)\n",
    "            pflip = torch.argmax(pred_flip, dim=-1)\n",
    "            \n",
    "            correct_rotations += (exp_rotated == prot).sum().item()\n",
    "            correct_flips += (exp_flipped == pflip).sum().item()\n",
    "\n",
    "            pbar.update(curr_batch_size)\n",
    "\n",
    "\n",
    "    result = {\n",
    "        \"loss\": np.sum(losses) / total_inputs,\n",
    "        \"correct_rotations\": correct_rotations / total_inputs,\n",
    "        \"correct_flips\": correct_flips / total_inputs,\n",
    "    }\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ffa44b27-61f1-48c8-a742-5b1726497ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adamp import AdamP\n",
    "\n",
    "epochs = 10\n",
    "lr = 5e-4\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = AdamP(model.parameters(), lr=lr)\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, \n",
    "                        steps_per_epoch=len(train_data_loader), epochs=epochs,anneal_strategy='linear')\n",
    "\n",
    "max_grad_norm = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d74f228-3436-4509-814e-340f1f0e5d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrolypolyvg295\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.31<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">rot_flip</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/rolypolyvg295/ocr\" target=\"_blank\">https://wandb.ai/rolypolyvg295/ocr</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/rolypolyvg295/ocr/runs/35dxgj8g\" target=\"_blank\">https://wandb.ai/rolypolyvg295/ocr/runs/35dxgj8g</a><br/>\n",
       "                Run data is saved locally in <code>/opt/ml/team/gj/code/wandb/run-20210604_012942-35dxgj8g</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<wandb.wandb_torch.TorchGraph at 0x7fca80defb50>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(project='ocr',\n",
    "        entity='rolypolyvg295',\n",
    "        tags=['rot_flip'], name='rot_flip',\n",
    ")\n",
    "wandb.watch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43115723-b552-4c77-b17a-efc750f42188",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train (Train):   0%|          | 0/80000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid (Validation):   0%|          | 0/80000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Train      Loss: 4.240153553152084\n",
      "Train  Rot Accr: 0.529175\n",
      "Train Flip Accr: 0.542175\n",
      "  Val      Loss: 6.380821299448609\n",
      "  Val  Rot Accr: 0.4929\n",
      "  Val Flip Accr: 0.6119375\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train (Train):   0%|          | 0/80000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid (Validation):   0%|          | 0/80000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Train      Loss: 1.3518753125853837\n",
      "Train  Rot Accr: 0.5782625\n",
      "Train Flip Accr: 0.612775\n",
      "  Val      Loss: 1.4414706039965153\n",
      "  Val  Rot Accr: 0.5495875\n",
      "  Val Flip Accr: 0.6290625\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train (Train):   0%|          | 0/80000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid (Validation):   0%|          | 0/80000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n",
      "Train      Loss: 1.255347069900483\n",
      "Train  Rot Accr: 0.6441875\n",
      "Train Flip Accr: 0.6855875\n",
      "  Val      Loss: 1.439285270076245\n",
      "  Val  Rot Accr: 0.5619375\n",
      "  Val Flip Accr: 0.74165\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train (Train):   0%|          | 0/80000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid (Validation):   0%|          | 0/80000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n",
      "Train      Loss: 0.9874531002484263\n",
      "Train  Rot Accr: 0.7132125\n",
      "Train Flip Accr: 0.770525\n",
      "  Val      Loss: 1.212342948173359\n",
      "  Val  Rot Accr: 0.6861125\n",
      "  Val Flip Accr: 0.81125\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train (Train):   0%|          | 0/80000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid (Validation):   0%|          | 0/80000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n",
      "Train      Loss: 0.8447270188182593\n",
      "Train  Rot Accr: 0.757475\n",
      "Train Flip Accr: 0.8053875\n",
      "  Val      Loss: 1.1723052965298295\n",
      "  Val  Rot Accr: 0.6949\n",
      "  Val Flip Accr: 0.819125\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train (Train):   0%|          | 0/80000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid (Validation):   0%|          | 0/80000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n",
      "Train      Loss: 0.7748444154765435\n",
      "Train  Rot Accr: 0.7849\n",
      "Train Flip Accr: 0.826825\n",
      "  Val      Loss: 1.2406106685530394\n",
      "  Val  Rot Accr: 0.7199875\n",
      "  Val Flip Accr: 0.8292\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train (Train):   0%|          | 0/80000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "valid (Validation):   0%|          | 0/80000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6\n",
      "Train      Loss: 0.7003111157268286\n",
      "Train  Rot Accr: 0.8086125\n",
      "Train Flip Accr: 0.8420625\n",
      "  Val      Loss: 1.524997286292538\n",
      "  Val  Rot Accr: 0.7514625\n",
      "  Val Flip Accr: 0.824525\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a99b3205e6f4a53995295f391d1a1a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train (Train):   0%|          | 0/80000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-6840228abbc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0muse_amp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     )\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-cdaabe63a9f4>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(data_loader, model, epoch_text, criterion, optimizer, lr_scheduler, max_grad_norm, device, use_amp, train)\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0;31m# Clip gradients, it returns the total norm of all parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 nn.utils.clip_grad_norm_(\n\u001b[0;32m---> 57\u001b[0;31m                     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                 )\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mclip_coef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mclip_coef\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mclip_coef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mclip_coef\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[1;32m   1291\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m             \u001b[0m_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# noqa: C416 TODO: rewrite as list(range(m))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1293\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdim\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m     \u001b[0;31m# TODO: when https://github.com/pytorch/pytorch/issues/33782 is fixed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    train_result = run_epoch(\n",
    "        train_data_loader,\n",
    "        model,\n",
    "        'train',\n",
    "        criterion,\n",
    "        optimizer, \n",
    "        lr_scheduler,\n",
    "        max_grad_norm,\n",
    "        device,\n",
    "        use_amp=True,\n",
    "        train=True,\n",
    "    )\n",
    "    \n",
    "    val_result = run_epoch(\n",
    "        train_data_loader,\n",
    "        model,\n",
    "        'valid',\n",
    "        criterion,\n",
    "        optimizer, \n",
    "        lr_scheduler,\n",
    "        max_grad_norm,\n",
    "        device,\n",
    "        use_amp=True,\n",
    "        train=False,\n",
    "    )\n",
    "    \n",
    "    train_loss = train_result['loss']\n",
    "    train_correct_rotations = train_result['correct_rotations']\n",
    "    train_correct_flips = train_result['correct_flips']\n",
    "    \n",
    "    val_loss = val_result['loss']\n",
    "    val_correct_rotations = val_result['correct_rotations']\n",
    "    val_correct_flips = val_result['correct_flips']\n",
    "    \n",
    "\n",
    "    print(f'Epoch {epoch}')\n",
    "    print(f'Train      Loss: {train_loss}')\n",
    "    print(f'Train  Rot Accr: {train_correct_rotations}')\n",
    "    print(f'Train Flip Accr: {train_correct_flips}')\n",
    "    print(f'  Val      Loss: {val_loss}')\n",
    "    print(f'  Val  Rot Accr: {val_correct_rotations}')\n",
    "    print(f'  Val Flip Accr: {val_correct_flips}')\n",
    "    print()\n",
    "    \n",
    "    logging_stuff = {\n",
    "        'train_loss':train_loss,\n",
    "        'train_correct_rotations':train_correct_rotations,\n",
    "        'train_correct_flips':train_correct_flips,\n",
    "        'val_loss':val_loss,\n",
    "        'val_correct_rotations':val_correct_rotations,\n",
    "        'val_correct_flips':val_correct_flips,\n",
    "    }\n",
    "    wandb.log(logging_stuff, step=epoch)\n",
    "    torch.save(model.state_dict(), f'/opt/ml/input/data/rot_model/latest.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9e7b89-d2f0-41d9-b13f-5a5ffdcbb4c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3484f7-6f47-4a96-974b-c877cd752ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
